from decimal import Decimal
from typing import List, Optional, Dict
import logging

from django.db import connection
from django.conf import settings
from llama_index.core import Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from .vector_db import VectorDBConnection


class ChromaDBLoader:
    def __init__(self):
        self.chroma_client = VectorDBConnection.get_client()
        
        # Handle model initialization with better error handling
        try:
            # Try standard initialization
            self.embedding_model = HuggingFaceEmbedding(model_name=settings.EMBEDDING_MODEL)
        except NotImplementedError as e:
            if "Cannot copy out of meta tensor" in str(e):
                # Fix for newer PyTorch versions that require to_empty()
                import torch
                from sentence_transformers import SentenceTransformer
                
                print("Using alternative model loading method for newer PyTorch versions")
                # Load model with device='meta' and then use to_empty()
                model = SentenceTransformer(settings.EMBEDDING_MODEL)
                if hasattr(model, "to_empty"):
                    try:
                        # For PyTorch 2.0+
                        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                        model = model.to_empty(device=device)
                        model.eval()
                        self.embedding_model = HuggingFaceEmbedding(model=model)
                    except Exception as inner_e:
                        print(f"Failed with to_empty() method: {inner_e}")
                        # Last resort - try with CPU only and no_grad
                        with torch.no_grad():
                            model = SentenceTransformer(settings.EMBEDDING_MODEL, device="cpu")
                            self.embedding_model = HuggingFaceEmbedding(model=model)
                else:
                    # Fallback to CPU only as last resort
                    print("Model doesn't have to_empty() method, falling back to CPU")
                    with torch.no_grad():
                        model = SentenceTransformer(settings.EMBEDDING_MODEL, device="cpu")
                        self.embedding_model = HuggingFaceEmbedding(model=model)
            else:
                # Re-raise if it's a different error
                raise

    def fetch_data(self, table_name: str, columns: List[str]):
        with connection.cursor() as cursor:
            query = f"SELECT {', '.join(columns)} FROM {table_name};"
            cursor.execute(query)
            column_names = [col[0] for col in cursor.description]
            result = [dict(zip(column_names, row)) for row in cursor.fetchall()]
            return result

    def load_data(
            self,
            table: str,
            sql_table: str,
            id_column: str,
            text_columns: List[str],
            metadata_columns: Optional[List[str]] = None,
            column_names: Optional[Dict[str, str]] = None
    ):
        columns_set = {id_column}
        columns_set.update(text_columns)
        if metadata_columns:
            columns_set.update(metadata_columns)
        columns = list(columns_set)

        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{sql_table}' –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫: {columns} ...")
        rows = self.fetch_data(sql_table, columns)
        if not rows:
            print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ '{sql_table}'!")
            return

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(rows)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{sql_table}'!")

        documents = []
        for row in rows:
            doc_id = str(row[id_column])
            texts = []
            for col in text_columns:
                value = row.get(col)
                if value:
                    if column_names and col in column_names.keys():
                        if column_names[col] == "":
                            texts.append(str(value))
                        else:
                            texts.append(f"{column_names[col]}: {value}")
                    else:
                        texts.append(str(value))
            text_content = "\n".join(texts)

            metadata = {}
            if metadata_columns:
                for col in metadata_columns:
                    value = row.get(col)
                    if isinstance(value, Decimal):
                        value = float(value)
                    metadata[col] = value
            metadata["id"] = row.get(id_column)
            documents.append(Document(doc_id=doc_id, text=text_content, metadata=metadata))

        collection = self.chroma_client.get_or_create_collection(table)

        doc_texts = [doc.text.lower() for doc in documents]
        try:
            embeddings = self.embedding_model._get_text_embeddings(doc_texts)
            
            ids = [doc.doc_id for doc in documents]
            metadatas = [doc.metadata for doc in documents]
            collection.add(
                ids=ids,
                documents=doc_texts,
                metadatas=metadatas,
                embeddings=embeddings
            )

            num_records = len(collection.get()["ids"])
            print(f"üìä –ó–∞–ø–∏—Å–µ–π –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{table}' ChromaDB: {num_records}")
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –≤ ChromaDB!")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é: {e}")
            raise